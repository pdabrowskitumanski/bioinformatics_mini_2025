{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chemical Space, QSAR, and Model Pitfalls\n",
    "\n",
    "## 90-Minute Laboratory\n",
    "\n",
    "This notebook explores fundamental concepts in computational drug discovery:\n",
    "\n",
    "1. **Chemical Space Exploration** â€” How molecules are represented and visualized\n",
    "2. **Activity Cliffs** â€” Pairs of structurally similar molecules with drastically different activities\n",
    "3. **QSAR Modeling** â€” Quantitative Structure-Activity Relationship prediction\n",
    "4. **Dataset Splitting Strategies** â€” Why the choice of train/test split matters dramatically\n",
    "5. **Model Explainability** â€” Understanding what drives predictions\n",
    "\n",
    "### Dataset\n",
    "We use the **SARS-CoV-2 Main Protease (Mpro)** inhibitor dataset from the COVID Moonshot project:\n",
    "- Source: [QSAR Activity Cliff Experiments](https://github.com/MarkusFerdinandDablander/QSAR-activity-cliff-experiments)\n",
    "- Target: IC50 values for Mpro inhibition\n",
    "- Molecules: ~1000 compounds with experimental measurements\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Exercises Overview\n",
    "\n",
    "| Exercise | Task | Deliverable |\n",
    "|----------|------|-------------|\n",
    "| 1.1 | Explore data and identify censoring threshold | Plot + threshold value |\n",
    "| 1.2 | Clean data and compute descriptors | Cleaned DataFrame |\n",
    "| 2.1 | Compute fingerprints and UMAP | UMAP visualization |\n",
    "| 3.1 | Find activity cliffs | List of cliff pairs + visualization |\n",
    "| 4.1 | Implement scaffold split | Split function |\n",
    "| 4.2 | Implement Butina clustering split | Split function |\n",
    "| 4.3 | Compare 4 splitting strategies | Comparison table + plots |\n",
    "| 5.1 | Analyze results and identify hardest split | Summary with interpretation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "# Core scientific stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# RDKit for cheminformatics\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs, Descriptors\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.ML.Cluster import Butina\n",
    "from rdkit.Chem.Draw import MolsToGridImage\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Dimensionality reduction\n",
    "import umap\n",
    "\n",
    "# Display utilities\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Global Parameters\n",
    "\n",
    "Key configuration values for reproducibility:\n",
    "- **FP_RADIUS / FP_BITS**: Morgan fingerprint parameters (radius=2 gives ECFP4-like fingerprints)\n",
    "- **RANDOM_SEED**: For reproducibility across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GLOBAL PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "CSV_URL = (\n",
    "    \"https://raw.githubusercontent.com/\"\n",
    "    \"MarkusFerdinandDablander/QSAR-activity-cliff-experiments/main/\"\n",
    "    \"data/postera_sars_cov_2_mpro/molecule_data_clean.csv\"\n",
    ")\n",
    "\n",
    "FP_RADIUS = 2                 # Morgan fingerprint radius (2 = ECFP4-like)\n",
    "FP_BITS = 2048                # Number of bits in fingerprint vector\n",
    "RANDOM_SEED = 42              # For reproducibility\n",
    "\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Cleaning\n",
    "\n",
    "### Why pKi instead of IC50?\n",
    "\n",
    "IC50 values span several orders of magnitude (nM to mM), making them difficult to model directly. We convert to **pKi** (negative log of IC50 in molar):\n",
    "\n",
    "$$\\text{pKi} = -\\log_{10}(\\text{IC50}_M)$$\n",
    "\n",
    "This transformation:\n",
    "- Converts multiplicative relationships to additive ones\n",
    "- Makes the distribution more normal\n",
    "- Puts values on a more intuitive scale (higher = more potent)\n",
    "\n",
    "### Censored Measurements\n",
    "\n",
    "Many compounds have IC50 reported as \">100 Î¼M\" (meaning: too weak to measure accurately). These **right-censored** values are problematic because:\n",
    "1. We don't know the true value â€” only that it exceeds a threshold\n",
    "2. Including them as if they were exact measurements biases the model\n",
    "3. They cluster at the threshold, creating artificial patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PROVIDED HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_dataset(url: str) -> pd.DataFrame:\n",
    "    \"\"\"Load the SARS-CoV-2 Mpro inhibitor dataset from GitHub.\"\"\"\n",
    "    df = pd.read_csv(url)\n",
    "    assert \"SMILES\" in df.columns, \"Missing SMILES column\"\n",
    "    assert \"f_avg_IC50 [uM]\" in df.columns, \"Missing IC50 column\"\n",
    "    \n",
    "    df = df[[\"SMILES\", \"f_avg_IC50 [uM]\"]].copy()\n",
    "    df.rename(columns={\"f_avg_IC50 [uM]\": \"IC50_uM\"}, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ic50_to_pki(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert IC50 (Î¼M) to pKi = -log10(IC50 in M).\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"IC50_M\"] = df[\"IC50_uM\"] * 1e-6\n",
    "    df[\"pKi\"] = -np.log10(df[\"IC50_M\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def fingerprints_to_array(fps: list) -> np.ndarray:\n",
    "    \"\"\"Convert RDKit fingerprints to numpy array.\"\"\"\n",
    "    return np.array([np.array(fp) for fp in fps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Exercise 1.1: Explore Data and Identify Censoring Threshold\n",
    "\n",
    "**Task**: Load the dataset, convert IC50 to pKi, and plot the distribution. Identify the censoring threshold from the data.\n",
    "\n",
    "**Expected Deliverable**:\n",
    "1. A histogram of pKi values showing the distribution\n",
    "2. Identification of the censoring threshold (the IC50 value above which measurements are \"censored\")\n",
    "3. Brief explanation of how you identified it\n",
    "\n",
    "**Hints**:\n",
    "- Look for suspicious spikes or clustering in the distribution\n",
    "- Check the maximum IC50 values â€” what's the pattern?\n",
    "- The censoring threshold creates an artificial \"pile-up\" of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXERCISE 1.1: Load and explore the data\n",
    "# ============================================================================\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = load_dataset(CSV_URL)\n",
    "print(f\"Loaded {len(df)} molecules\")\n",
    "print(f\"IC50 range: {df['IC50_uM'].min():.3f} - {df['IC50_uM'].max():.3f} Î¼M\")\n",
    "\n",
    "# Step 2: Convert to pKi\n",
    "df = ic50_to_pki(df)\n",
    "\n",
    "# TODO: Plot the pKi distribution (histogram)\n",
    "# Hint: Use plt.hist() or sns.histplot()\n",
    "\n",
    "\n",
    "# TODO: Examine the IC50 values to identify the censoring threshold\n",
    "# Hint: Look at df[\"IC50_uM\"].value_counts() or df[\"IC50_uM\"].describe()\n",
    "\n",
    "\n",
    "# TODO: Set your identified censoring threshold\n",
    "IC50_CENSOR_THRESHOLD = None  # TODO: Replace with the threshold you identified (in Î¼M)\n",
    "\n",
    "print(f\"\\nðŸ“Š Identified censoring threshold: {IC50_CENSOR_THRESHOLD} Î¼M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Exercise 1.2: Clean Data and Compute Descriptors\n",
    "\n",
    "**Task**: Remove censored measurements, parse SMILES to RDKit molecules, and add molecular descriptors.\n",
    "\n",
    "**Expected Deliverable**:\n",
    "1. A cleaned DataFrame with only non-censored molecules\n",
    "2. RDKit molecule objects in a \"mol\" column\n",
    "3. Basic molecular descriptors (MW, logP, HBD, HBA)\n",
    "4. A histogram showing the pKi distribution AFTER removing censored values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXERCISE 1.2: Clean data and compute descriptors\n",
    "# ============================================================================\n",
    "\n",
    "def remove_censored_measurements(df: pd.DataFrame, threshold: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove compounds with IC50 above the censoring threshold.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    - Filter out rows where IC50_uM > threshold\n",
    "    - Print how many molecules were removed\n",
    "    - Return the filtered DataFrame\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "def smiles_to_mol(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse SMILES strings into RDKit molecule objects.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    - Use Chem.MolFromSmiles() to parse each SMILES\n",
    "    - Handle invalid SMILES (they return None)\n",
    "    - Add a \"mol\" column to the DataFrame\n",
    "    - Return only rows with valid molecules\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "def add_basic_descriptors(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate basic molecular descriptors.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    - Add columns: MW (molecular weight), logP, HBD (H-bond donors), HBA (H-bond acceptors)\n",
    "    - Use Descriptors.MolWt, Descriptors.MolLogP, Descriptors.NumHDonors, Descriptors.NumHAcceptors\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "# Apply the cleaning pipeline\n",
    "# TODO: Uncomment and run after implementing the functions above\n",
    "# df = remove_censored_measurements(df, threshold=IC50_CENSOR_THRESHOLD)\n",
    "# df = smiles_to_mol(df)\n",
    "# df = add_basic_descriptors(df)\n",
    "\n",
    "# TODO: Plot the pKi distribution AFTER cleaning\n",
    "\n",
    "# TODO: Print basic statistics\n",
    "# print(df[[\"pKi\", \"MW\", \"logP\", \"HBD\", \"HBA\"]].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Molecular Fingerprints & Chemical Space Visualization\n",
    "\n",
    "### Morgan Fingerprints (ECFP)\n",
    "\n",
    "Morgan fingerprints encode circular substructures around each atom:\n",
    "- **Radius 2** captures atom environments up to 2 bonds away (similar to ECFP4)\n",
    "- Each substructure is hashed to a bit position in a fixed-length vector\n",
    "- Similarity between molecules is measured using **Tanimoto coefficient**\n",
    "\n",
    "### UMAP Projection\n",
    "\n",
    "UMAP (Uniform Manifold Approximation and Projection) reduces the 2048-dimensional fingerprint space to 2D for visualization:\n",
    "- Uses Jaccard distance (equivalent to 1 - Tanimoto for binary vectors)\n",
    "- Preserves local neighborhood structure\n",
    "- Reveals clusters of similar molecules\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Exercise 2.1: Compute Fingerprints and UMAP Projection\n",
    "\n",
    "**Task**: Compute Morgan fingerprints for all molecules and visualize chemical space using UMAP.\n",
    "\n",
    "**Expected Deliverable**:\n",
    "1. A list of Morgan fingerprints for all molecules\n",
    "2. A 2D UMAP projection colored by pKi\n",
    "3. Brief observations about the chemical space structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXERCISE 2.1: Compute fingerprints and UMAP projection\n",
    "# ============================================================================\n",
    "\n",
    "def compute_fingerprints(df: pd.DataFrame) -> list:\n",
    "    \"\"\"\n",
    "    Compute Morgan fingerprints for all molecules.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    - Use AllChem.GetMorganFingerprintAsBitVect(mol, FP_RADIUS, nBits=FP_BITS)\n",
    "    - Return a list of fingerprints\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "def umap_projection(fps: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Project fingerprints to 2D using UMAP.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    - Convert fingerprints to numpy array using fingerprints_to_array()\n",
    "    - Create a UMAP reducer with metric=\"jaccard\"\n",
    "    - Return the 2D embedding\n",
    "    \n",
    "    Hint: Use umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"jaccard\", random_state=RANDOM_SEED)\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "def plot_umap(emb: np.ndarray, values: np.ndarray, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot UMAP embedding colored by a value (e.g., pKi).\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    - Use plt.scatter with c=values and a colormap\n",
    "    - Add colorbar, labels, title\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO: Compute fingerprints\n",
    "# fps = compute_fingerprints(df)\n",
    "\n",
    "# TODO: Run UMAP projection\n",
    "# emb = umap_projection(fps)\n",
    "\n",
    "# TODO: Visualize chemical space\n",
    "# plot_umap(emb, df[\"pKi\"].values, \"Chemical Space (UMAP) â€” colored by pKi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activity Cliffs\n",
    "\n",
    "### What are Activity Cliffs?\n",
    "\n",
    "An **activity cliff** is a pair of molecules that are:\n",
    "1. **Structurally very similar** (high Tanimoto similarity, e.g., >0.95)\n",
    "2. **Dramatically different in activity** (large Î”pKi, e.g., >2.0 units = 100Ã— difference in potency)\n",
    "\n",
    "### Why do Activity Cliffs Matter?\n",
    "\n",
    "- They reveal **SAR (Structure-Activity Relationship)** hotspots\n",
    "- Small chemical changes can have huge biological effects\n",
    "- They are **notoriously difficult for ML models** to predict\n",
    "- A model that memorizes training data will fail on cliffs in the test set\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Exercise 3.1: Find and Visualize Activity Cliffs\n",
    "\n",
    "**Task**: Identify pairs of molecules that form activity cliffs and visualize them.\n",
    "\n",
    "**Expected Deliverable**:\n",
    "1. A function to find activity cliffs given similarity and Î”pKi thresholds\n",
    "2. Number of activity cliffs found\n",
    "3. Visualization of the top 5 most dramatic cliffs (molecule pairs side by side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXERCISE 3.1: Find and visualize activity cliffs\n",
    "# ============================================================================\n",
    "\n",
    "def find_activity_cliffs(\n",
    "    df: pd.DataFrame, \n",
    "    fps: list, \n",
    "    sim_threshold: float = 0.95, \n",
    "    delta_pki_threshold: float = 2.0\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Find pairs of molecules that are structurally similar but have very different activities.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    - For each pair of molecules, compute Tanimoto similarity\n",
    "    - If similarity >= sim_threshold AND |Î”pKi| >= delta_pki_threshold, it's a cliff\n",
    "    - Return list of tuples: (idx_i, idx_j, similarity, delta_pKi)\n",
    "    \n",
    "    Hint: Use DataStructs.BulkTanimotoSimilarity(fps[i], fps) to compute similarity\n",
    "          of molecule i to all others efficiently\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "def visualize_activity_cliffs(df: pd.DataFrame, cliffs: list, n_pairs: int = 5):\n",
    "    \"\"\"\n",
    "    Display pairs of molecules that form activity cliffs.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    - Sort cliffs by delta_pKi (most dramatic first)\n",
    "    - Use MolsToGridImage to display pairs\n",
    "    - Include pKi values and similarity in legends\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "# TODO: Find activity cliffs\n",
    "# cliffs = find_activity_cliffs(df, fps, sim_threshold=0.95, delta_pki_threshold=2.0)\n",
    "\n",
    "# TODO: Visualize top cliffs\n",
    "# visualize_activity_cliffs(df, cliffs, n_pairs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaffold Analysis\n",
    "\n",
    "### Murcko Scaffolds\n",
    "\n",
    "The **Murcko scaffold** is the core ring system of a molecule (stripping side chains):\n",
    "- Useful for grouping molecules by chemical series\n",
    "- Important for **scaffold-based splitting** â€” ensuring the model sees novel chemotypes at test time\n",
    "\n",
    "### Why Scaffold Splitting?\n",
    "\n",
    "Random splits often leak information:\n",
    "- Training and test sets may contain molecules from the same series\n",
    "- The model can \"cheat\" by memorizing series-level patterns\n",
    "- This leads to **overly optimistic** performance estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SCAFFOLD ANALYSIS (provided)\n",
    "# ============================================================================\n",
    "\n",
    "def compute_scaffolds(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Extract Murcko scaffolds for all molecules.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"scaffold\"] = df[\"mol\"].apply(\n",
    "        lambda m: Chem.MolToSmiles(MurckoScaffold.GetScaffoldForMol(m))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "# TODO: Compute scaffolds (uncomment after Exercise 1.2 is complete)\n",
    "# df = compute_scaffolds(df)\n",
    "# n_scaffolds = df[\"scaffold\"].nunique()\n",
    "# print(f\"[INFO] Found {n_scaffolds} unique scaffolds among {len(df)} molecules\")\n",
    "# print(f\"       Average molecules per scaffold: {len(df) / n_scaffolds:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset Splitting Strategies\n",
    "\n",
    "We compare **4 different splitting strategies** to understand how the choice affects model evaluation:\n",
    "\n",
    "### 1. Random Split\n",
    "- Molecules are randomly assigned to train/test\n",
    "- **Problem**: Similar molecules (even from the same series) can appear in both sets\n",
    "- **Result**: Overly optimistic performance\n",
    "\n",
    "### 2. Scaffold Split\n",
    "- Entire scaffolds (chemical series) go to either train OR test\n",
    "- **Effect**: Tests generalization to new chemotypes\n",
    "- **Result**: More realistic but still may group similar scaffolds\n",
    "\n",
    "### 3. Butina Clustering Split\n",
    "- Clusters molecules by Tanimoto similarity using Butina algorithm\n",
    "- Entire clusters go to train or test\n",
    "- **Effect**: Ensures structural novelty in test set\n",
    "- **Result**: Harder than scaffold split\n",
    "\n",
    "### 4. UMAP-based Split\n",
    "- Uses UMAP 2D projection to identify spatially separated regions\n",
    "- Test set comes from distinct regions of chemical space\n",
    "- **Effect**: Maximum structural novelty\n",
    "- **Result**: The hardest, most realistic test of generalization\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Exercise 4.1: Implement Scaffold Split\n",
    "\n",
    "**Task**: Implement a function that splits data by scaffold.\n",
    "\n",
    "**Expected Deliverable**: A function that returns train/test boolean masks ensuring no scaffold appears in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXERCISE 4.1: Implement scaffold split\n",
    "# ============================================================================\n",
    "\n",
    "def random_split(X: np.ndarray, y: np.ndarray, test_size: float = 0.2):\n",
    "    \"\"\"Standard random train/test split (provided).\"\"\"\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=RANDOM_SEED)\n",
    "\n",
    "\n",
    "def scaffold_split(df: pd.DataFrame, test_size: float = 0.2):\n",
    "    \"\"\"\n",
    "    Split by scaffold: entire chemical series go to train or test.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    - Get unique scaffolds from df[\"scaffold\"]\n",
    "    - Shuffle them randomly\n",
    "    - Assign first test_size fraction of scaffolds to test set\n",
    "    - Return (train_mask, test_mask) as boolean numpy arrays\n",
    "    \n",
    "    Returns:\n",
    "        train_mask: boolean array, True for training samples\n",
    "        test_mask: boolean array, True for test samples\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Exercise 4.2: Implement Butina Clustering Split\n",
    "\n",
    "**Task**: Implement a function that clusters molecules using Butina algorithm and splits by clusters.\n",
    "\n",
    "**Expected Deliverable**: A function that returns train/test boolean masks ensuring no cluster appears in both sets.\n",
    "\n",
    "**Hints**:\n",
    "- Use `DataStructs.BulkTanimotoSimilarity` to compute pairwise similarities\n",
    "- Use `Butina.ClusterData(distances, n_molecules, cutoff, isDistData=True)`\n",
    "- The Butina algorithm needs a distance matrix in condensed form (lower triangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXERCISE 4.2: Implement Butina clustering split\n",
    "# ============================================================================\n",
    "\n",
    "def butina_split(fps: list, cutoff: float = 0.6, test_size: float = 0.2):\n",
    "    \"\"\"\n",
    "    Cluster molecules using Butina algorithm, then split by clusters.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    - Compute pairwise Tanimoto distances (1 - similarity)\n",
    "    - Cluster using Butina.ClusterData()\n",
    "    - Shuffle clusters and assign test_size fraction to test set\n",
    "    - Return (train_mask, test_mask) as boolean numpy arrays\n",
    "    \n",
    "    Args:\n",
    "        fps: list of fingerprints\n",
    "        cutoff: distance cutoff for clustering (default 0.6 = 40% similarity)\n",
    "        test_size: fraction of clusters to use for test\n",
    "        \n",
    "    Returns:\n",
    "        train_mask: boolean array, True for training samples\n",
    "        test_mask: boolean array, True for test samples\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UMAP-BASED SPLIT (provided)\n",
    "# ============================================================================\n",
    "\n",
    "def umap_split(emb: np.ndarray, test_size: float = 0.2, n_regions: int = 10):\n",
    "    \"\"\"\n",
    "    Split by UMAP regions: divide 2D space into grid cells, \n",
    "    assign entire cells to train or test.\n",
    "    \n",
    "    This ensures test molecules come from distinct regions of chemical space.\n",
    "    \"\"\"\n",
    "    n = len(emb)\n",
    "    \n",
    "    # Create grid-based regions\n",
    "    x_bins = np.linspace(emb[:, 0].min(), emb[:, 0].max(), n_regions + 1)\n",
    "    y_bins = np.linspace(emb[:, 1].min(), emb[:, 1].max(), n_regions + 1)\n",
    "    \n",
    "    # Assign each point to a region\n",
    "    x_idx = np.digitize(emb[:, 0], x_bins[:-1]) - 1\n",
    "    y_idx = np.digitize(emb[:, 1], y_bins[:-1]) - 1\n",
    "    region_ids = x_idx * n_regions + y_idx\n",
    "    \n",
    "    # Get unique regions and shuffle\n",
    "    unique_regions = np.unique(region_ids)\n",
    "    np.random.shuffle(unique_regions)\n",
    "    \n",
    "    # Assign regions to test set until we reach target size\n",
    "    test_mask = np.zeros(n, dtype=bool)\n",
    "    current_test_size = 0\n",
    "    target_test_size = int(test_size * n)\n",
    "    \n",
    "    for region in unique_regions:\n",
    "        if current_test_size >= target_test_size:\n",
    "            break\n",
    "        region_mask = region_ids == region\n",
    "        test_mask |= region_mask\n",
    "        current_test_size = test_mask.sum()\n",
    "    \n",
    "    return ~test_mask, test_mask\n",
    "\n",
    "\n",
    "print(\"âœ… UMAP split function provided\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. QSAR Modeling\n",
    "\n",
    "### Model: XGBoost Regressor\n",
    "\n",
    "We use **XGBoost** (Gradient Boosted Trees) for QSAR modeling:\n",
    "- Works well with sparse fingerprint features\n",
    "- Handles non-linear relationships\n",
    "- Fast training on medium-sized datasets\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "1. **RMSE** (Root Mean Squared Error): Measures prediction accuracy in pKi units\n",
    "2. **Spearman Ï** (Rank Correlation): Measures ability to rank compounds correctly\n",
    "   - More important for virtual screening where we care about ranking, not absolute values\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Exercise 4.3: Compare All 4 Splitting Strategies\n",
    "\n",
    "**Task**: Train XGBoost models using all 4 splits and compare their performance.\n",
    "\n",
    "**Expected Deliverable**:\n",
    "1. A comparison table showing RMSE and Spearman Ï for each split\n",
    "2. Bar chart visualization of the results\n",
    "3. Identification of which split is hardest/easiest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# QSAR MODELING FUNCTIONS (provided)\n",
    "# ============================================================================\n",
    "\n",
    "def train_xgb(X_train: np.ndarray, y_train: np.ndarray) -> XGBRegressor:\n",
    "    \"\"\"Train an XGBoost regressor.\"\"\"\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        random_state=RANDOM_SEED,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model: XGBRegressor, X_test: np.ndarray, y_test: np.ndarray) -> dict:\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    rho, pval = spearmanr(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"rmse\": rmse,\n",
    "        \"spearman_rho\": rho,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"âœ… Modeling functions provided\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXERCISE 4.3: Compare all 4 splitting strategies\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Prepare feature matrix and target\n",
    "# X = fingerprints_to_array(fps)\n",
    "# y = df[\"pKi\"].values\n",
    "\n",
    "# TODO: Store results for comparison\n",
    "# results = {}\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# TODO: 1. RANDOM SPLIT\n",
    "# -------------------------------------------------------------------------\n",
    "# X_train, X_test, y_train, y_test = random_split(X, y)\n",
    "# model_random = train_xgb(X_train, y_train)\n",
    "# results[\"Random\"] = evaluate_model(model_random, X_test, y_test)\n",
    "# print(f\"Random: RMSE={results['Random']['rmse']:.3f}, Ï={results['Random']['spearman_rho']:.3f}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# TODO: 2. SCAFFOLD SPLIT\n",
    "# -------------------------------------------------------------------------\n",
    "# train_mask, test_mask = scaffold_split(df)\n",
    "# X_train, X_test = X[train_mask], X[test_mask]\n",
    "# y_train, y_test = y[train_mask], y[test_mask]\n",
    "# model_scaffold = train_xgb(X_train, y_train)\n",
    "# results[\"Scaffold\"] = evaluate_model(model_scaffold, X_test, y_test)\n",
    "# print(f\"Scaffold: RMSE={results['Scaffold']['rmse']:.3f}, Ï={results['Scaffold']['spearman_rho']:.3f}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# TODO: 3. BUTINA CLUSTERING SPLIT\n",
    "# -------------------------------------------------------------------------\n",
    "# train_mask, test_mask = butina_split(fps, cutoff=0.6)\n",
    "# X_train, X_test = X[train_mask], X[test_mask]\n",
    "# y_train, y_test = y[train_mask], y[test_mask]\n",
    "# model_butina = train_xgb(X_train, y_train)\n",
    "# results[\"Butina\"] = evaluate_model(model_butina, X_test, y_test)\n",
    "# print(f\"Butina: RMSE={results['Butina']['rmse']:.3f}, Ï={results['Butina']['spearman_rho']:.3f}\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# TODO: 4. UMAP-BASED SPLIT\n",
    "# -------------------------------------------------------------------------\n",
    "# train_mask, test_mask = umap_split(emb, n_regions=8)\n",
    "# X_train, X_test = X[train_mask], X[test_mask]\n",
    "# y_train, y_test = y[train_mask], y[test_mask]\n",
    "# model_umap = train_xgb(X_train, y_train)\n",
    "# results[\"UMAP\"] = evaluate_model(model_umap, X_test, y_test)\n",
    "# print(f\"UMAP: RMSE={results['UMAP']['rmse']:.3f}, Ï={results['UMAP']['spearman_rho']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Exercise 5.1: Analyze Results and Conclusions\n",
    "\n",
    "**Task**: Create visualizations comparing the 4 splits and identify key insights.\n",
    "\n",
    "**Expected Deliverable**:\n",
    "1. A comparison DataFrame sorted by difficulty (highest RMSE first)\n",
    "2. Bar charts showing RMSE and Spearman Ï for each split\n",
    "3. Scatter plots of predicted vs. experimental values for each split\n",
    "4. Written answers to the following questions:\n",
    "\n",
    "### Questions to Answer:\n",
    "\n",
    "1. **Which split was hardest? Which was easiest?** Why do you think this is?\n",
    "\n",
    "2. **What is the performance gap** between random and the hardest split? What does this tell you about using random splits for QSAR evaluation?\n",
    "\n",
    "3. **How do activity cliffs relate** to the difficulty of structure-aware splits?\n",
    "\n",
    "4. **What recommendations** would you give for evaluating QSAR models in a real drug discovery project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXERCISE 5.1: Analyze results and create visualizations\n",
    "# ============================================================================\n",
    "\n",
    "# TODO: Create a comparison DataFrame\n",
    "# comparison_df = pd.DataFrame({\n",
    "#     \"Split\": list(results.keys()),\n",
    "#     \"RMSE\": [results[k][\"rmse\"] for k in results],\n",
    "#     \"Spearman Ï\": [results[k][\"spearman_rho\"] for k in results]\n",
    "# }).sort_values(\"RMSE\", ascending=False)\n",
    "# print(comparison_df)\n",
    "\n",
    "# TODO: Identify hardest and easiest splits\n",
    "# hardest = comparison_df.iloc[0][\"Split\"]\n",
    "# easiest = comparison_df.iloc[-1][\"Split\"]\n",
    "# print(f\"\\nHardest: {hardest}, Easiest: {easiest}\")\n",
    "\n",
    "# TODO: Create bar charts comparing RMSE and Spearman Ï\n",
    "\n",
    "\n",
    "# TODO: Create scatter plots for each split (2x2 grid)\n",
    "\n",
    "\n",
    "# TODO: Calculate and print the performance gap\n",
    "# gap_rmse = comparison_df.iloc[0][\"RMSE\"] - comparison_df.iloc[-1][\"RMSE\"]\n",
    "# print(f\"RMSE gap: {gap_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answers:\n",
    "\n",
    "**1. Which split was hardest? Which was easiest? Why?**\n",
    "\n",
    "*TODO: Write your answer here*\n",
    "\n",
    "\n",
    "\n",
    "**2. What is the performance gap? What does this tell you?**\n",
    "\n",
    "*TODO: Write your answer here*\n",
    "\n",
    "\n",
    "\n",
    "**3. How do activity cliffs relate to split difficulty?**\n",
    "\n",
    "*TODO: Write your answer here*\n",
    "\n",
    "\n",
    "\n",
    "**4. Recommendations for real QSAR evaluation?**\n",
    "\n",
    "*TODO: Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Splitting strategy dramatically affects apparent model performance**\n",
    "   - Random splits are overly optimistic\n",
    "   - Structure-aware splits (scaffold, Butina, UMAP) give more realistic estimates\n",
    "\n",
    "2. **Activity cliffs are a fundamental challenge**\n",
    "   - Small structural changes can cause large activity differences\n",
    "   - Fingerprint-based models struggle with cliffs\n",
    "   - This is a key limitation of 2D representations\n",
    "\n",
    "3. **The \"applicability domain\" matters**\n",
    "   - Models work best within the chemical space of training data\n",
    "   - Predictions for novel chemotypes should be treated with caution\n",
    "\n",
    "### Recommendations for Real-World QSAR\n",
    "\n",
    "- Always use **structure-aware splitting** for validation\n",
    "- Report performance on multiple split types\n",
    "- Be skeptical of models with very high random-split performance\n",
    "- Consider the **applicability domain** when deploying models\n",
    "- Use **ensemble methods** to estimate prediction uncertainty\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Checklist\n",
    "\n",
    "Before submitting, make sure you have completed:\n",
    "\n",
    "- [ ] Exercise 1.1: Identified censoring threshold from data\n",
    "- [ ] Exercise 1.2: Implemented data cleaning functions\n",
    "- [ ] Exercise 2.1: Computed fingerprints and UMAP projection\n",
    "- [ ] Exercise 3.1: Found and visualized activity cliffs\n",
    "- [ ] Exercise 4.1: Implemented scaffold split\n",
    "- [ ] Exercise 4.2: Implemented Butina clustering split\n",
    "- [ ] Exercise 4.3: Compared all 4 splitting strategies\n",
    "- [ ] Exercise 5.1: Analyzed results and answered questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Exercise 1.2: Clean Data and Compute Descriptors\n",
    "\n",
    "**Task**: Remove censored measurements, parse SMILES to RDKit molecules, and add molecular descriptors.\n",
    "\n",
    "**Expected Deliverable**:\n",
    "1. A cleaned DataFrame with only non-censored molecules\n",
    "2. RDKit molecule objects in a \"mol\" column\n",
    "3. Basic molecular descriptors (MW, logP, HBD, HBA)\n",
    "4. A histogram showing the pKi distribution AFTER removing censored values\n",
    "\n",
    "**Functions to implement**:\n",
    "- `remove_censored_measurements(df, threshold)`: Remove rows with IC50 > threshold\n",
    "- `smiles_to_mol(df)`: Parse SMILES strings to RDKit molecules\n",
    "- `add_basic_descriptors(df)`: Add MW, logP, HBD, HBA columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
